{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: DNA binding classifier\n",
    "\n",
    "again, help from Isaiah Hazelwood and ChatGPT \n",
    "\n",
    "loading prerequisites and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import nn \n",
    "from nn import io \n",
    "from nn import preprocess\n",
    "import sklearn\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_seqs = io.read_text_file(\"./data/rap1-lieb-positives.txt\")\n",
    "negative_seqs = io.read_fasta_file(\"./data/yeast-upstream-1k-negative.fa\")\n",
    "#negative sequences are 1000 characters while positives are 17 characters long\n",
    "#need to have same length of positive and negative examples\n",
    "#need to randomly select 17-character long substring from each negative sequence\n",
    "np.random.seed(42)\n",
    "negative_seqs_trimmed = []\n",
    "for negative_seq in negative_seqs:\n",
    "    i = np.random.randint(len(negative_seq) - 17 + 1) #random starting position\n",
    "    negative_seqs_trimmed.append(negative_seq[i:i+17])\n",
    "\n",
    "#generate a full sequence list \n",
    "seqs = positive_seqs + negative_seqs_trimmed\n",
    "labels = ([True] * len(positive_seqs)) + ([False] * len(negative_seqs_trimmed))\n",
    "\n",
    "#randomly sample sequences to account for class imbalance \n",
    "sampled_seqs, sampled_labels = preprocess.sample_seqs(seqs, labels)\n",
    "\n",
    "#now I'm encoding the sequences\n",
    "sample_seqs_encoded = preprocess.one_hot_encode_seqs(sampled_seqs)\n",
    "\n",
    "#randomly split sequences into training and validation sets\n",
    "indexes_permuted = np.random.permutation(len(sampled_labels))\n",
    "train_indices = indexes_permuted[:len(sample_seqs_encoded) * 7 // 10]\n",
    "val_indices = indexes_permuted[len(sample_seqs_encoded) * 7 // 10:]\n",
    "\"\"\"\n",
    "because encoded means sequence length * 4 (encoder length) = 17 * 4 = 68\n",
    "so list of sequences having length 68 as encoded amino acids, represented as flat arrays. \n",
    "without reshaping, I would get (num_samples, 68) after calling np.array([sampled_seqs_encoded[i] for i in train_indices])\n",
    "each row representing one sequence (sample) and each column represents one encoded character\n",
    "in ML the neural network expects a clearly defined 3-D shape as input (batch_size, sequence_length, num_channels)\n",
    "so number of samples in a batch -> if -1, numpy infers this automatically, \n",
    "sequence_lenght is 68 in my case, num_channels is number of features per position and because\n",
    "each position in my sequence is represented by a single numeric value (one-hot encoded aa) this would typically be 1\n",
    "for numerical sequences channel usually is 1. \n",
    "\"\"\"\n",
    "seqs_train = np.array([sample_seqs_encoded[i] for i in train_indices]).reshape((-1, 68, 1))\n",
    "#labels_train without reshaping (500,0), need to reshape to represent in 3D\n",
    "labels_train = np.array([sampled_labels[i] for i in train_indices]).astype(np.float64).reshape(-1, 1, 1)\n",
    "seqs_val = np.array([sample_seqs_encoded[i] for i in val_indices]).reshape(-1, 68, 1)\n",
    "labels_val = np.array([sampled_labels[i] for i in val_indices]).astype(np.float64).reshape(-1, 1, 1)\n",
    "\n",
    "#sampling number of less frequent class equal to number of data points from most frequent class (with replacement) ensures that all \n",
    "#the data is being used in thhe training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#define the classifier\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dna_classifier = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_dim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m17\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_dim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mactivation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_dim}\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_dim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mactivation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_dim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_dim\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mactivation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msigmoid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.015\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbinary_cross_entropy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03mbinary cross entropy measures how different two probability distributions are. \u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03min my case predicted vs actual labels. calculates loss between predicted output and actual target values. \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[33;03mBCE is specifically designed for classification.\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/final-nn/nn/nn.py:56\u001b[39m, in \u001b[36mNeuralNetwork.__init__\u001b[39m\u001b[34m(self, nn_arch, lr, seed, batch_size, epochs, loss_function)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m._batch_size = batch_size\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Initialize the parameter dictionary for use in training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28mself\u001b[39m._param_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/final-nn/nn/nn.py:80\u001b[39m, in \u001b[36mNeuralNetwork._init_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.arch):\n\u001b[32m     79\u001b[39m     layer_idx = idx + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     input_dim = \u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_dim\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     81\u001b[39m     output_dim = layer[\u001b[33m'\u001b[39m\u001b[33moutput_dim\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m#creates weights from a standard normal distribution (mean = 0, std = 1) and scales them down by multiplying by 0.1.\u001b[39;00m\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m#generates a matrix of shape output_dim, input_dim, because need to connect every input neuron to an output neuron. \u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'input_dim'"
     ]
    }
   ],
   "source": [
    "#define the classifier\n",
    "dna_classifier = nn.NeuralNetwork([{\"input_dim\":17 * 4, \"output_dim\": 20, \"activation\": \"relu\"},\n",
    "                                  {\"input_dim}\": 20, \"output_dim\":5, \"activation\":\"relu\"},\n",
    "                                  {\"input_dim\":5, \"output_dim\":1, \"activation\":\"sigmoid\"}],\n",
    "                                  lr=0.015,\n",
    "                                  seed=42,\n",
    "                                  batch_size=20, \n",
    "                                  epochs=7,\n",
    "                                  loss_function=\"binary_cross_entropy\")\n",
    "\n",
    "\"\"\"\n",
    "binary cross entropy measures how different two probability distributions are. \n",
    "in my case predicted vs actual labels. calculates loss between predicted output and actual target values. \n",
    "BCE is ideal because outputs represent probabilities between 0 and 1, as it gives a confidence estimate\n",
    "on if that given sequence contains a motif.\n",
    "I am dealing with binary classification so BCE is exactly designed for this.\n",
    "\n",
    "MSE would assume continuous values and not probabilities...\n",
    "BCE is specifically designed for classification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the classifier\n",
    "per_epoch_loss_train, per_epoch_loss_val = dna_classifier.fit(\n",
    "    seqs_train, \n",
    "    labels_train,\n",
    "    seqs_val,\n",
    "    labels_val\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "the training sequences and training labels are used directly for the forward pass (calculate predictions)\n",
    "the backward pass (calculate gradients and update parameters)\n",
    "the validation sequences and labels are used only to calculate the validation loss to see how well \n",
    "the model performs. no backward pass and no weight updates occur on the validation set. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot training and validation loss by epoch \n",
    "fig, axs = plt.subplots()\n",
    "axs.plot(per_epoch_loss_train, label=\"training loss\")\n",
    "axs.plot(per_epoch_loss_val, label=\"validation loss\")\n",
    "axs.set_xlabel(\"Epoch\")\n",
    "axs.set_ylabel(\"mean reconstruction loss\")\n",
    "axs.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prediction = dna_classifier.predict(seqs_val)\n",
    "#val prediction outputs values between 0 and 1. \n",
    "#rounding these values makes it either 1 (True) or 0 (False)\n",
    "#np.mean() computes the fravtion of correct predictions. \n",
    "val_accuracy = np.mean(np.round(val_prediction) == labels_val)\n",
    "print(f\"final accuracy on validation set: {val_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
