{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: DNA binding classifier\n",
    "\n",
    "again, help from Isaiah Hazelwood and ChatGPT \n",
    "\n",
    "loading prerequisites and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import nn \n",
    "from nn import io \n",
    "from nn import preprocess\n",
    "import sklearn\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_seqs = io.read_text_file(\"./data/rap1-lieb-positives.txt\")\n",
    "negative_seqs = io.read_fasta_file(\"./data/yeast-upstream-1k-negative.fa\")\n",
    "#negative sequences are 1000 characters while positives are 17 characters long\n",
    "#need to have same length of positive and negative examples\n",
    "#need to randomly select 17-character long substring from each negative sequence\n",
    "np.random.seed(42)\n",
    "negative_seqs_trimmed = []\n",
    "for negative_seq in negative_seqs:\n",
    "    i = np.random.randint(len(negative_seq) - 17 + 1) #random starting position\n",
    "    negative_seqs_trimmed.append(negative_seq[i:i+17])\n",
    "\n",
    "#generate a full sequence list \n",
    "seqs = positive_seqs + negative_seqs_trimmed\n",
    "labels = ([True] * len(positive_seqs)) + ([False] * len(negative_seqs_trimmed))\n",
    "\n",
    "#randomly sample sequences to account for class imbalance \n",
    "sampled_seqs, sampled_labels = preprocess.sample_seqs(seqs, labels)\n",
    "\n",
    "#now I'm encoding the sequences\n",
    "sample_seqs_encoded = preprocess.one_hot_encode_seqs(sampled_seqs)\n",
    "\n",
    "#randomly split sequences into training and validation sets\n",
    "indexes_permuted = np.random.permutation(len(sampled_labels))\n",
    "train_indices = indexes_permuted[:len(sample_seqs_encoded) * 7 // 10]\n",
    "val_indices = indexes_permuted[len(sample_seqs_encoded) * 7 // 10:]\n",
    "\"\"\"\n",
    "because encoded means sequence length * 4 (encoder length) = 17 * 4 = 68\n",
    "so list of sequences having length 68 as encoded amino acids, represented as flat arrays. \n",
    "without reshaping, I would get (num_samples, 68) after calling np.array([sampled_seqs_encoded[i] for i in train_indices])\n",
    "each row representing one sequence (sample) and each column represents one encoded character\n",
    "in ML the neural network expects a clearly defined 3-D shape as input (batch_size, sequence_length, num_channels)\n",
    "so number of samples in a batch -> if -1, numpy infers this automatically, \n",
    "sequence_lenght is 68 in my case, num_channels is number of features per position and because\n",
    "each position in my sequence is represented by a single numeric value (one-hot encoded aa) this would typically be 1\n",
    "for numerical sequences channel usually is 1. \n",
    "\"\"\"\n",
    "seqs_train = np.array([sample_seqs_encoded[i] for i in train_indices]).reshape(-1, 68)\n",
    "#labels_train without reshaping (500,0), need to reshape to represent in 3D\n",
    "labels_train = np.array([sampled_labels[i] for i in train_indices]).astype(np.float64).reshape(-1, 1)\n",
    "seqs_val = np.array([sample_seqs_encoded[i] for i in val_indices]).reshape(-1, 68)\n",
    "labels_val = np.array([sampled_labels[i] for i in val_indices]).astype(np.float64).reshape(-1, 1)\n",
    "\n",
    "#sampling number of less frequent class equal to number of data points from most frequent class (with replacement) ensures that all \n",
    "#the data is being used in thhe training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbinary cross entropy measures how different two probability distributions are. \\nin my case predicted vs actual labels. calculates loss between predicted output and actual target values. \\nBCE is ideal because outputs represent probabilities between 0 and 1, as it gives a confidence estimate\\non if that given sequence contains a motif.\\nI am dealing with binary classification so BCE is exactly designed for this.\\n\\nMSE would assume continuous values and not probabilities...\\nBCE is specifically designed for classification.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the classifier\n",
    "dna_classifier = nn.NeuralNetwork([{\"input_dim\":17 * 4, \"output_dim\": 20, \"activation\": \"relu\"},\n",
    "                                  {\"input_dim\": 20, \"output_dim\":5, \"activation\":\"relu\"},\n",
    "                                  {\"input_dim\":5, \"output_dim\":1, \"activation\":\"sigmoid\"}],\n",
    "                                  lr=0.015,\n",
    "                                  seed=42,\n",
    "                                  batch_size=20, \n",
    "                                  epochs=7,\n",
    "                                  loss_function=\"binary_cross_entropy\")\n",
    "\n",
    "\"\"\"\n",
    "binary cross entropy measures how different two probability distributions are. \n",
    "in my case predicted vs actual labels. calculates loss between predicted output and actual target values. \n",
    "BCE is ideal because outputs represent probabilities between 0 and 1, as it gives a confidence estimate\n",
    "on if that given sequence contains a motif.\n",
    "I am dealing with binary classification so BCE is exactly designed for this.\n",
    "\n",
    "MSE would assume continuous values and not probabilities...\n",
    "BCE is specifically designed for classification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 20 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#train the classifier\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m per_epoch_loss_train, per_epoch_loss_val = \u001b[43mdna_classifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseqs_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseqs_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels_val\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03mthe training sequences and training labels are used directly for the forward pass (calculate predictions)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03mthe backward pass (calculate gradients and update parameters)\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03mthe validation sequences and labels are used only to calculate the validation loss to see how well \u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03mthe model performs. no backward pass and no weight updates occur on the validation set. \u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/final-nn/nn/nn.py:334\u001b[39m, in \u001b[36mNeuralNetwork.fit\u001b[39m\u001b[34m(self, X_train, y_train, X_val, y_val)\u001b[39m\n\u001b[32m    332\u001b[39m     y_train_batch = y_train[batch_start:batch_stop, :]\n\u001b[32m    333\u001b[39m     y_pred_batch, forward_cache = \u001b[38;5;28mself\u001b[39m.forward(X_train_batch)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     batch_grads = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_params(batch_grads)\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m#calculating the accuracies for this epok\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/final-nn/nn/nn.py:259\u001b[39m, in \u001b[36mNeuralNetwork.backprop\u001b[39m\u001b[34m(self, y, y_hat, cache)\u001b[39m\n\u001b[32m    256\u001b[39m A_prev = cache[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mA\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] \u001b[38;5;66;03m#A0 is input, A1 is after layer 1... \u001b[39;00m\n\u001b[32m    257\u001b[39m activation = layer[\u001b[33m\"\u001b[39m\u001b[33mactivation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m dA_prev, dW_curr, db_curr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_backprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mW_curr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb_curr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mZ_curr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mA_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurr_dA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactivation\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;66;03m#save gradients\u001b[39;00m\n\u001b[32m    269\u001b[39m grad_dict[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = dW_curr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/final-nn/nn/nn.py:209\u001b[39m, in \u001b[36mNeuralNetwork._single_backprop\u001b[39m\u001b[34m(self, W_curr, b_curr, Z_curr, A_prev, dA_curr, activation_curr)\u001b[39m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown activation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation_curr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m#tells me how much the output A_prev influenced the loss. this is the signal I send back to the previous layer. \u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m#this is the starting dA for the previous layer in my next single_backprop call. \u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m dA_prev = \u001b[43mW_curr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mdZ_curr\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m#tells me how much the loss would change if I change each weight slightly. \u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m#for every weight between neuron i and neuron j it says how much would it change the final loss if I tweak the weight up or down a bit. \u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m#this it the key ingredient for updating the weights. W = W - learning_rate * dW\u001b[39;00m\n\u001b[32m    214\u001b[39m dW_curr = dZ_curr @ A_prev.transpose((\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 20 is different from 1)"
     ]
    }
   ],
   "source": [
    "#train the classifier\n",
    "per_epoch_loss_train, per_epoch_loss_val = dna_classifier.fit(\n",
    "    seqs_train, \n",
    "    labels_train,\n",
    "    seqs_val,\n",
    "    labels_val\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "the training sequences and training labels are used directly for the forward pass (calculate predictions)\n",
    "the backward pass (calculate gradients and update parameters)\n",
    "the validation sequences and labels are used only to calculate the validation loss to see how well \n",
    "the model performs. no backward pass and no weight updates occur on the validation set. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot training and validation loss by epoch \n",
    "fig, axs = plt.subplots()\n",
    "axs.plot(per_epoch_loss_train, label=\"training loss\")\n",
    "axs.plot(per_epoch_loss_val, label=\"validation loss\")\n",
    "axs.set_xlabel(\"Epoch\")\n",
    "axs.set_ylabel(\"mean reconstruction loss\")\n",
    "axs.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prediction = dna_classifier.predict(seqs_val)\n",
    "#val prediction outputs values between 0 and 1. \n",
    "#rounding these values makes it either 1 (True) or 0 (False)\n",
    "#np.mean() computes the fravtion of correct predictions. \n",
    "val_accuracy = np.mean(np.round(val_prediction) == labels_val)\n",
    "print(f\"final accuracy on validation set: {val_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
